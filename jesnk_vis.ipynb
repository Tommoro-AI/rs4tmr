{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initalized env with init_env\n",
      "obj.name:Can, obj_pos:(np.float64(0.18232603946299536), np.float64(-0.16043680533894142), np.float64(0.8600000000000001)), obj_quat:[0.82358043 0.         0.         0.56719951]\n",
      "robot0_joint_pos\n",
      "robot0_joint_pos_cos\n",
      "robot0_joint_pos_sin\n",
      "robot0_joint_vel\n",
      "robot0_eef_pos\n",
      "robot0_eef_quat\n",
      "robot0_eef_vel_lin\n",
      "robot0_eef_vel_ang\n",
      "robot0_gripper_qpos\n",
      "robot0_gripper_qvel\n",
      "agentview_image\n",
      "agentview_depth\n",
      "world_pose_in_gripper\n",
      "Milk_pos\n",
      "Milk_quat\n",
      "Milk_to_robot0_eef_pos\n",
      "Milk_to_robot0_eef_quat\n",
      "Bread_pos\n",
      "Bread_quat\n",
      "Bread_to_robot0_eef_pos\n",
      "Bread_to_robot0_eef_quat\n",
      "Cereal_pos\n",
      "Cereal_quat\n",
      "Cereal_to_robot0_eef_pos\n",
      "Cereal_to_robot0_eef_quat\n",
      "Can_pos\n",
      "Can_quat\n",
      "Can_to_robot0_eef_pos\n",
      "Can_to_robot0_eef_quat\n",
      "obj.name:Can, obj_pos:(np.float64(0.18232603946299536), np.float64(-0.16043680533894142), np.float64(0.8600000000000001)), obj_quat:[0.82358043 0.         0.         0.56719951]\n",
      "obj.name:Can, obj_pos:(np.float64(0.18232603946299536), np.float64(-0.16043680533894142), np.float64(0.8600000000000001)), obj_quat:[0.82358043 0.         0.         0.56719951]\n",
      "obj.name:Can, obj_pos:(np.float64(0.18232603946299536), np.float64(-0.16043680533894142), np.float64(0.8600000000000001)), obj_quat:[0.82358043 0.         0.         0.56719951]\n"
     ]
    }
   ],
   "source": [
    "from my_utils import init_env\n",
    "import mediapy as media\n",
    "env = init_env(wandb_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj.name:Can, obj_pos:(np.float64(0.18232603946299536), np.float64(-0.16043680533894142), np.float64(0.8600000000000001)), obj_quat:[0.82358043 0.         0.         0.56719951]\n",
      "(51,)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs[0].shape)\n",
    "#env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.82330929e-01, -1.60444739e-01,  8.60049060e-01,  1.01499951e-04,\n",
       "         2.86367925e-06,  5.67199675e-01,  8.23580305e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -4.62736731e-01, -1.75366954e+00,\n",
       "         2.46353836e+00, -2.25814027e+00, -1.60262569e+00, -1.97096943e+00,\n",
       "        -4.55423569e-03,  4.66794841e-02, -8.24490000e-02,  3.19293302e-02,\n",
       "        -1.23902292e-02, -4.18278282e-02, -1.62235140e-01, -1.27381766e-01,\n",
       "         9.92336382e-01, -9.99325406e-01,  3.11289610e-02,  1.68813781e-02,\n",
       "        -9.73345478e-03,  2.53249655e-02, -1.56067148e-02,  1.46576935e-02,\n",
       "        -1.13620419e-02,  2.86288036e-03,  3.75209867e-02,  1.68853982e-01,\n",
       "        -3.12082981e-01, -2.21873466e-02,  1.68822876e-01, -3.12543534e-01,\n",
       "        -2.21542338e-02, -1.08678359e+00,  2.24025552e+00, -1.06142166e+00,\n",
       "        -1.08701484e+00,  2.22533782e+00, -1.06056026e+00]),\n",
       " np.float64(0.00012080240937375476),\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/rs4tmr/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.image_states to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.image_states` for environment variables or `env.get_wrapper_attr('image_states')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agentview_image': array([[[203, 190, 182],\n",
       "         [211, 198, 189],\n",
       "         [208, 196, 185],\n",
       "         ...,\n",
       "         [186, 177, 168],\n",
       "         [191, 182, 173],\n",
       "         [187, 178, 169]],\n",
       " \n",
       "        [[203, 190, 182],\n",
       "         [208, 195, 186],\n",
       "         [210, 197, 187],\n",
       "         ...,\n",
       "         [191, 182, 173],\n",
       "         [193, 184, 175],\n",
       "         [179, 170, 161]],\n",
       " \n",
       "        [[205, 193, 185],\n",
       "         [205, 192, 184],\n",
       "         [208, 195, 186],\n",
       "         ...,\n",
       "         [203, 195, 186],\n",
       "         [191, 183, 174],\n",
       "         [184, 175, 166]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[117, 115, 111],\n",
       "         [118, 116, 112],\n",
       "         [118, 116, 112],\n",
       "         ...,\n",
       "         [117, 115, 111],\n",
       "         [117, 115, 111],\n",
       "         [116, 114, 111]],\n",
       " \n",
       "        [[118, 117, 113],\n",
       "         [118, 116, 113],\n",
       "         [118, 116, 113],\n",
       "         ...,\n",
       "         [118, 116, 112],\n",
       "         [117, 116, 112],\n",
       "         [117, 115, 112]],\n",
       " \n",
       "        [[118, 116, 113],\n",
       "         [118, 116, 112],\n",
       "         [117, 115, 112],\n",
       "         ...,\n",
       "         [118, 116, 113],\n",
       "         [117, 115, 112],\n",
       "         [117, 115, 112]]], dtype=uint8),\n",
       " 'agentview_depth': array([[[0.9939623 ],\n",
       "         [0.9939623 ],\n",
       "         [0.9939623 ],\n",
       "         ...,\n",
       "         [0.9939623 ],\n",
       "         [0.9939623 ],\n",
       "         [0.9939623 ]],\n",
       " \n",
       "        [[0.9939762 ],\n",
       "         [0.9939762 ],\n",
       "         [0.9939762 ],\n",
       "         ...,\n",
       "         [0.9939762 ],\n",
       "         [0.9939762 ],\n",
       "         [0.9939762 ]],\n",
       " \n",
       "        [[0.99399006],\n",
       "         [0.99399006],\n",
       "         [0.99399006],\n",
       "         ...,\n",
       "         [0.99399006],\n",
       "         [0.99399006],\n",
       "         [0.99399006]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.99649286],\n",
       "         [0.99649286],\n",
       "         [0.99649286],\n",
       "         ...,\n",
       "         [0.99649286],\n",
       "         [0.99649286],\n",
       "         [0.99649286]],\n",
       " \n",
       "        [[0.99648476],\n",
       "         [0.99648476],\n",
       "         [0.99648476],\n",
       "         ...,\n",
       "         [0.99648476],\n",
       "         [0.99648476],\n",
       "         [0.99648476]],\n",
       " \n",
       "        [[0.99647665],\n",
       "         [0.99647665],\n",
       "         [0.99647665],\n",
       "         ...,\n",
       "         [0.99647665],\n",
       "         [0.99647665],\n",
       "         [0.99647665]]], dtype=float32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.image_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get observation according key name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      2\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample())\n\u001b[1;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39mget_obs_according_to_key(obs[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrobot0_joint_pos_cos\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "obs = env.step(env.action_space.sample())\n",
    "env.get_obs_according_to_key(obs[0],'robot0_joint_pos_cos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Checkpoint load and Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /research/rs4tmr/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "# 필요한 클래스 및 함수 임포트\n",
    "from cleanrl.cleanrl.ppo_continuous_action import Agent, Args, jesnk_make_env\n",
    "import cv2\n",
    "\n",
    "def load_model_and_evaluate(model_path, env_id=\"HalfCheetah-v4\", num_episodes=10, seed=1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    저장된 모델을 불러와 환경에서 평가를 수행하는 함수\n",
    "    \"\"\"\n",
    "    visualize = True\n",
    "    frames = []\n",
    "    \n",
    "    # Argument 설정\n",
    "    args = Args()\n",
    "    args.env_id = env_id\n",
    "    args.seed = seed\n",
    "    args.gamma = gamma\n",
    "\n",
    "    # 환경 생성\n",
    "    env = gym.vector.SyncVectorEnv(\n",
    "        [jesnk_make_env(args.env_id, i, args.capture_video, \"evaluation\", args.gamma, wandb_enabled=False, active_image=True) for i in range(1)]\n",
    "    )\n",
    "    \n",
    "    # 디바이스 설정 (cuda가 가능하면 cuda 사용)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        print(\"Using CUDA\")\n",
    "    else :\n",
    "        assert device == torch.device(\"cpu\")\n",
    "\n",
    "    # Agent 초기화 및 모델 불러오기\n",
    "    agent = Agent(env).to(device)\n",
    "    agent.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    agent.eval()  # 평가 모드로 전환\n",
    "\n",
    "    # 평가 수행\n",
    "    total_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset(seed=args.seed)\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        image_frame = env.envs[0].image_states['agentview_image']\n",
    "        image_frame = np.array(image_frame[::-1, :, :], dtype=np.uint8)  # 명시적으로 numpy 배열로 변환\n",
    "        frames.append(image_frame)\n",
    "        # convert image_frame cv2 image\n",
    "        #image_frame = cv2.cvtColor(image_frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        frames.append(image_frame)\n",
    "        \n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            with torch.no_grad():\n",
    "                obs = torch.Tensor(obs).to(device)\n",
    "                action, _, _, _ = agent.get_action_and_value(obs)\n",
    "            obs, reward, terminations, truncations, infos = env.step(action.cpu().numpy())\n",
    "            done = np.logical_or(terminations, truncations).any()\n",
    "            episode_reward += reward[0]  # 첫 번째 환경의 보상 합산\n",
    "            \n",
    "            # 새로운 프레임 가져오기 및 변환\n",
    "            image_frame = env.envs[0].image_states['agentview_image']\n",
    "            image_frame = np.array(image_frame[::-1, :, :], dtype=np.uint8)  # numpy 배열로 변환\n",
    "\n",
    "            #image_frame = cv2.cvtColor(image_frame, cv2.COLOR_RGB2BGR)\n",
    "            # draw text on image_frame episode reward, reward, small text\n",
    "            cv2.putText(image_frame, f\"Episode Reward: {episode_reward:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            cv2.putText(image_frame, f\"Reward: {reward[0]:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            \n",
    "            frames.append(image_frame)\n",
    "            \n",
    "\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {episode_reward}\")\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # 평균 리턴 출력\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {avg_reward}\")\n",
    "    \n",
    "    media.show_video(frames, fps=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initalized env with init_env\n",
      "fix_object:False\n",
      "active_rewards: ['r', 'g', 'l', 'h']\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_485881/3224904554.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.load_state_dict(torch.load(model_path, map_location=device))\n",
      "/root/anaconda3/envs/rs4tmr/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.image_states to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.image_states` for environment variables or `env.get_wrapper_attr('image_states')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'agentview_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 저장된 모델 불러와 평가하기\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/research/rs4tmr/cleanrl/cleanrl/runs/tr__ppo_continuous_action__s1__2024-09-25_06-51-36/ppo_continuous_action_198656.cleanrl_model\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 모델 경로 지정\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mload_model_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHalfCheetah-v4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 47\u001b[0m, in \u001b[0;36mload_model_and_evaluate\u001b[0;34m(model_path, env_id, num_episodes, seed, gamma)\u001b[0m\n\u001b[1;32m     44\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     45\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 47\u001b[0m image_frame \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magentview_image\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     48\u001b[0m image_frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image_frame[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)  \u001b[38;5;66;03m# 명시적으로 numpy 배열로 변환\u001b[39;00m\n\u001b[1;32m     49\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(image_frame)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'agentview_image'"
     ]
    }
   ],
   "source": [
    "# 저장된 모델 불러와 평가하기\n",
    "\n",
    "model_path = \"/research/rs4tmr/cleanrl/cleanrl/runs/tr__ppo_continuous_action__s1__2024-09-25_06-51-36/ppo_continuous_action_198656.cleanrl_model\"  # 모델 경로 지정\n",
    "load_model_and_evaluate(model_path, env_id=\"HalfCheetah-v4\", num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /research/rs4tmr/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robosuite environment maked: <class 'robosuite.environments.manipulation.tmr_pp.TmrPickPlaceCan'> <robosuite.environments.manipulation.tmr_pp.TmrPickPlaceCan object at 0x7ff04aaedeb0> ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_action_dim', '_check_grasp', '_check_robot_configuration', '_check_success', '_construct_objects', '_construct_visual_objects', '_create_camera_sensors', '_create_obj_sensors', '_create_segementation_sensor', '_destroy_sim', '_destroy_viewer', '_eef_xmat', '_eef_xpos', '_eef_xquat', '_get_observations', '_get_placement_initializer', '_gripper_to_target', '_initialize_sim', '_input2list', '_load_model', '_load_robots', '_obs_cache', '_observables', '_post_action', '_pre_action', '_reset_internal', '_setup_observables', '_setup_references', '_update_observables', '_visualizations', '_visualize_gripper_to_target', '_xml_processor', 'action_dim', 'action_spec', 'active_observables', 'add_observable', 'bin1_pos', 'bin2_pos', 'bin_size', 'camera_depths', 'camera_heights', 'camera_names', 'camera_segmentations', 'camera_widths', 'check_contact', 'clear_objects', 'close', 'close_renderer', 'control_freq', 'control_timestep', 'cur_time', 'deterministic_reset', 'done', 'edit_model_xml', 'enabled_observables', 'env_configuration', 'get_contacts', 'get_pixel_obs', 'hard_reset', 'has_offscreen_renderer', 'has_renderer', 'horizon', 'ignore_done', 'initialize_renderer', 'initialize_time', 'model', 'model_timestep', 'modify_observable', 'not_in_bin', 'num_cameras', 'num_robots', 'obj_body_id', 'obj_geom_id', 'obj_names', 'obj_to_use', 'object_id', 'object_id_to_sensors', 'object_to_id', 'objects', 'objects_in_bins', 'observation_modalities', 'observation_names', 'observation_spec', 'placement_initializer', 'render', 'render_camera', 'render_collision_mesh', 'render_gpu_device_id', 'render_visual_mesh', 'renderer', 'renderer_config', 'reset', 'reset_from_xml_string', 'reward', 'reward_scale', 'reward_shaping', 'robot_configs', 'robot_names', 'robots', 'set_camera_pos_quat', 'set_xml_processor', 'sim', 'sim_state_initial', 'single_object_mode', 'staged_rewards', 'step', 'table_friction', 'table_full_size', 'target_bin_placements', 'timestep', 'use_camera_obs', 'use_object_obs', 'viewer', 'viewer_get_obs', 'visual_objects', 'visualize', 'z_offset', 'z_rotation']\n",
      "27\n",
      "odict_keys(['robot0_joint_pos', 'robot0_joint_pos_cos', 'robot0_joint_pos_sin', 'robot0_joint_vel', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_eef_vel_lin', 'robot0_eef_vel_ang', 'robot0_gripper_qpos', 'robot0_gripper_qvel', 'world_pose_in_gripper', 'Milk_pos', 'Milk_quat', 'Milk_to_robot0_eef_pos', 'Milk_to_robot0_eef_quat', 'Bread_pos', 'Bread_quat', 'Bread_to_robot0_eef_pos', 'Bread_to_robot0_eef_quat', 'Cereal_pos', 'Cereal_quat', 'Cereal_to_robot0_eef_pos', 'Cereal_to_robot0_eef_quat', 'Can_pos', 'Can_quat', 'Can_to_robot0_eef_pos', 'Can_to_robot0_eef_quat'])\n"
     ]
    }
   ],
   "source": [
    "import robosuite as suite\n",
    "from robosuite.wrappers import GymWrapper\n",
    "\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n",
    "rsenv = suite.make(\n",
    "    \"TmrPickPlaceCan\",\n",
    "    robots=\"UR5e\",  # use UR5e robot\n",
    "    use_camera_obs=False,  # use pixel observations\n",
    "    has_offscreen_renderer=True,  # needed if using pixel obs\n",
    "    has_renderer=True,  # make sure we can render to the screen\n",
    "    reward_shaping=True,  # use dense rewards\n",
    "    control_freq=20,  # control should happen fast enough so that simulation looks smooth\n",
    ")\n",
    "\n",
    "# full_observable_list = [\n",
    "#     'robot0_joint_pos', \n",
    "#     'robot0_joint_pos_cos', \n",
    "#     'robot0_joint_pos_sin', \n",
    "#     'robot0_joint_vel', \n",
    "#     'robot0_eef_pos', \n",
    "#     'robot0_eef_quat', \n",
    "#     'robot0_eef_vel_lin', \n",
    "#     'robot0_eef_vel_ang', \n",
    "#     'robot0_gripper_qpos', \n",
    "#     'robot0_gripper_qvel', \n",
    "#     'agentview_image', \n",
    "#     'world_pose_in_gripper', \n",
    "#     'Milk_pos', \n",
    "#     'Milk_quat', \n",
    "#     'Milk_to_robot0_eef_pos', \n",
    "#     'Milk_to_robot0_eef_quat', \n",
    "#     'Bread_pos', \n",
    "#     'Bread_quat', \n",
    "#     'Bread_to_robot0_eef_pos', \n",
    "#     'Bread_to_robot0_eef_quat', \n",
    "#     'Cereal_pos', \n",
    "#     'Cereal_quat', \n",
    "#     'Cereal_to_robot0_eef_pos', \n",
    "#     'Cereal_to_robot0_eef_quat', \n",
    "#     'Can_pos', \n",
    "#     'Can_quat', \n",
    "#     'Can_to_robot0_eef_pos', \n",
    "#     'Can_to_robot0_eef_quat',\n",
    "# ]\n",
    "# for observable in full_observable_list:\n",
    "#     rsenv.modify_observable(observable, 'enabled', True)\n",
    "#     rsenv.modify_observable(observable, 'active', True)\n",
    "\n",
    "useless_observable_list = [\n",
    "    # 'robot0_joint_pos', \n",
    "    # 'robot0_joint_pos_cos', \n",
    "    # 'robot0_joint_pos_sin', \n",
    "    # 'robot0_joint_vel', \n",
    "    # 'robot0_eef_pos', \n",
    "    # 'robot0_eef_quat', \n",
    "    # 'robot0_eef_vel_lin', \n",
    "    # 'robot0_eef_vel_ang', \n",
    "    # 'robot0_gripper_qpos', \n",
    "    # 'robot0_gripper_qvel', \n",
    "    # 'agentview_image', \n",
    "    # 'world_pose_in_gripper', \n",
    "    # 'Milk_pos', \n",
    "    # 'Milk_quat', \n",
    "    # 'Milk_to_robot0_eef_pos', \n",
    "    # 'Milk_to_robot0_eef_quat', \n",
    "    # 'Bread_pos', \n",
    "    # 'Bread_quat', \n",
    "    # 'Bread_to_robot0_eef_pos', \n",
    "    # 'Bread_to_robot0_eef_quat', \n",
    "    # 'Cereal_pos', \n",
    "    # 'Cereal_quat', \n",
    "    # 'Cereal_to_robot0_eef_pos', \n",
    "    # 'Cereal_to_robot0_eef_quat', \n",
    "    # 'Can_pos', \n",
    "    # 'Can_quat', \n",
    "    # 'Can_to_robot0_eef_pos', \n",
    "    # 'Can_to_robot0_eef_quat',\n",
    "]\n",
    "for observable in useless_observable_list:\n",
    "    rsenv.modify_observable(observable, 'enabled', False)\n",
    "    rsenv.modify_observable(observable, 'active', False)\n",
    "\n",
    "print('Robosuite environment maked:',type(rsenv) , rsenv, dir(rsenv))\n",
    "print(len(rsenv._observables.keys()))\n",
    "print(rsenv._observables.keys())\n",
    "\n",
    "env = GymWrapper(\n",
    "    rsenv\n",
    ")\n",
    "\n",
    "env.reset(seed=0)\n",
    "\n",
    "new_env = Monitor(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[164, 155, 147],\n",
       "        [162, 153, 145],\n",
       "        [163, 154, 146],\n",
       "        ...,\n",
       "        [179, 170, 163],\n",
       "        [163, 154, 146],\n",
       "        [181, 172, 164]],\n",
       "\n",
       "       [[165, 156, 149],\n",
       "        [163, 154, 146],\n",
       "        [162, 153, 145],\n",
       "        ...,\n",
       "        [163, 155, 146],\n",
       "        [177, 169, 160],\n",
       "        [197, 188, 179]],\n",
       "\n",
       "       [[167, 158, 151],\n",
       "        [165, 156, 148],\n",
       "        [163, 154, 146],\n",
       "        ...,\n",
       "        [185, 177, 168],\n",
       "        [192, 183, 174],\n",
       "        [189, 180, 171]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[129, 127, 123],\n",
       "        [129, 127, 123],\n",
       "        [129, 128, 124],\n",
       "        ...,\n",
       "        [ 86,  85,  82],\n",
       "        [ 85,  84,  82],\n",
       "        [ 86,  84,  82]],\n",
       "\n",
       "       [[129, 128, 124],\n",
       "        [129, 127, 123],\n",
       "        [128, 127, 122],\n",
       "        ...,\n",
       "        [ 85,  84,  81],\n",
       "        [ 85,  84,  81],\n",
       "        [ 86,  85,  82]],\n",
       "\n",
       "       [[128, 126, 121],\n",
       "        [128, 125, 120],\n",
       "        [127, 124, 120],\n",
       "        ...,\n",
       "        [ 84,  82,  79],\n",
       "        [ 85,  83,  80],\n",
       "        [ 85,  84,  81]]], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsenv = suite.make(\n",
    "    \"TmrPickPlaceCan\",\n",
    "    robots=\"UR5e\",  # use UR5e robot\n",
    "    use_camera_obs=False,  # use pixel observations\n",
    "    has_offscreen_renderer=True,  # needed if using pixel obs\n",
    "    has_renderer=True,  # make sure we can render to the screen\n",
    "    reward_shaping=True,  # use dense rewards\n",
    "    control_freq=20,  # control should happen fast enough so that simulation looks smooth\n",
    ")\n",
    "rsenv.sim.render(width=640, height=480, camera_name='frontview', device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(new_env.render())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs4tmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
