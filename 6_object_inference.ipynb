{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 samples from can_pos_quat_1.npy\n",
      "Loaded 1000 samples from can_pos_quat_2.npy\n",
      "Loaded 1000 samples from can_pos_quat_3.npy\n",
      "Loaded 1000 samples from can_pos_quat_4.npy\n",
      "Concatenated can_pos_quat with shape (4000, 7)\n",
      "Concatenated depth_frames with shape (4000, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path = 'data'\n",
    "file_prefix_list = ['depth_frames', 'can_pos_quat']\n",
    "file_suffix_range = range(1, 5)\n",
    "\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data = {}\n",
    "data['depth_frames'] = []\n",
    "data['can_pos_quat'] = []\n",
    "# interate over all files\n",
    "for idx in file_suffix_range:\n",
    "    can_pos_quat_path = f\"can_pos_quat_{idx}.npy\"\n",
    "    depth_frames_path = f\"depth_frames_{idx}.npy\"\n",
    "    \n",
    "    can_pos_quat = np.load(os.path.join(path, can_pos_quat_path))\n",
    "    depth_frames = np.load(os.path.join(path, depth_frames_path))\n",
    "    print(f\"Loaded {can_pos_quat.shape[0]} samples from {can_pos_quat_path}\")\n",
    "    # concat\n",
    "    data['can_pos_quat'].append(can_pos_quat)\n",
    "    data['depth_frames'].append(depth_frames)\n",
    "    \n",
    "\n",
    "# concatenate all data\n",
    "data['can_pos_quat'] = np.concatenate(data['can_pos_quat'], axis=0)\n",
    "data['depth_frames'] = np.concatenate(data['depth_frames'], axis=0)\n",
    "\n",
    "print(f\"Concatenated can_pos_quat with shape {data['can_pos_quat'].shape}\")\n",
    "print(f\"Concatenated depth_frames with shape {data['depth_frames'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting the quaternion to only the first 3 elements. New shape: (4000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Post process\n",
    "\n",
    "# Cutting the quaternion to only the first 3 elements\n",
    "data['can_pos_quat'] = data['can_pos_quat'][:, :3]\n",
    "print(f\"Cutting the quaternion to only the first 3 elements. New shape: {data['can_pos_quat'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized can_pos_quat with mean [ 0.10136824 -0.24822153  0.86      ] and std [7.20919620e-02 1.00940761e-01 2.50910404e-14]\n"
     ]
    }
   ],
   "source": [
    "# Normalize can_pos_quat\n",
    "mean = data['can_pos_quat'].mean(axis=0)\n",
    "std = data['can_pos_quat'].std(axis=0)\n",
    "data['can_pos_quat'] = (data['can_pos_quat'] - mean) / std\n",
    "print(f\"Normalized can_pos_quat with mean {mean} and std {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data on device cuda\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset on GPU\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data['can_pos_quat'] = torch.tensor(data['can_pos_quat'], device=device)\n",
    "data['depth_frames'] = torch.tensor(data['depth_frames'], device=device)\n",
    "\n",
    "print(f\"Loaded data on device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149224/4281998498.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.depth_frames = torch.tensor(depth_frames, dtype=torch.float32).permute(0, 3, 1, 2)\n",
      "/tmp/ipykernel_4149224/4281998498.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.can_pos_quat = torch.tensor(can_pos_quat, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class DepthCanDataset(Dataset):\n",
    "    def __init__(self, depth_frames, can_pos_quat):\n",
    "        # Depth frames and corresponding position/quaternion data\n",
    "        # Permute depth frames from (batch, 256, 256, 1) to (batch, 1, 256, 256)\n",
    "        self.depth_frames = torch.tensor(depth_frames, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        self.can_pos_quat = torch.tensor(can_pos_quat, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.can_pos_quat)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        depth_frame = self.depth_frames[idx]\n",
    "        can_pos_quat = self.can_pos_quat[idx]\n",
    "        \n",
    "        return depth_frame, can_pos_quat\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = DepthCanDataset(data['depth_frames'], data['can_pos_quat'][:, :3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /data/jskang/rs4tmr/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "# 필요한 클래스 및 함수 임포트\n",
    "from cleanrl.cleanrl.ppo_continuous_action import Agent, Args, ppo_make_env\n",
    "import cv2\n",
    "import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_object_model(model,num_episodes,visualize=False, \n",
    "                          load_eval_data=False,\n",
    "                          success_threshold=0.01,\n",
    "                          ):\n",
    "\n",
    "    frames = []\n",
    "    # Argument 설정\n",
    "    task_id = 'pickplace'\n",
    "    gamma = 0.99\n",
    "    render_camera = ['birdview']#,'agentview'] #('frontview', 'birdview', 'agentview', 'robot0_robotview', 'robot0_eye_in_hand')\n",
    "    camera_names = render_camera\n",
    "    model.eval()\n",
    "\n",
    "    # 환경 생성\n",
    "    env = gym.vector.SyncVectorEnv(\n",
    "        [ppo_make_env(\n",
    "            task_id=task_id, \n",
    "            reward_shaping=True,\n",
    "            idx=0, \n",
    "            control_mode=\"OSC_POSITION\",\n",
    "            capture_video=False, \n",
    "            run_name=\"eval\", \n",
    "            gamma= gamma, \n",
    "            active_rewards=\"r\",\n",
    "            active_image=True, \n",
    "            fix_object=False,\n",
    "            wandb_enabled=False,\n",
    "            verbose=False,\n",
    "            control_freq=20,\n",
    "            render_camera=render_camera,\n",
    "            camera_names=camera_names,\n",
    "\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    def colorize_depth(frame):\n",
    "        # Assuming the depth image is in float32 and contains values representing distances.\n",
    "        # Normalize the depth image to 0-255 for visualization\n",
    "        min_depth = np.min(frame)\n",
    "        max_depth = np.max(frame)\n",
    "        normalized_depth = 255 * (frame - min_depth) / (max_depth - min_depth)\n",
    "        normalized_depth = normalized_depth.astype(np.uint8)\n",
    "\n",
    "        # Apply a colormap for better visualization (COLORMAP_JET is commonly used)\n",
    "        colorized_depth = cv2.applyColorMap(normalized_depth, cv2.COLORMAP_JET)\n",
    "        return colorized_depth\n",
    "\n",
    "\n",
    "    # 디바이스 설정 (cuda가 가능하면 cuda 사용)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        pass\n",
    "    else :\n",
    "        assert device == torch.device(\"cpu\")\n",
    "\n",
    "    # 평가 수행\n",
    "    viewer_image_key = 'birdview'+'_depth'\n",
    "\n",
    "    # generate samples\n",
    "    #model.to(device)\n",
    "    visualize = False\n",
    "    success_threshold = success_threshold\n",
    "    success_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if load_eval_data:\n",
    "            global_std = std.copy()\n",
    "            global_mean = mean.copy()\n",
    "            global_std = torch.tensor(global_std, device=device)\n",
    "            global_mean = torch.tensor(global_mean, device=device)\n",
    "            eval_data = {}\n",
    "\n",
    "            path = 'data'\n",
    "            if num_episodes == 100:\n",
    "                can_pos_quat_path = 'can_pos_quat_eval.npy'\n",
    "                depth_frames_path = 'depth_frames_eval.npy'\n",
    "            elif num_episodes == 1000:\n",
    "                can_pos_quat_path = 'can_pos_quat_eval_1000.npy'\n",
    "                depth_frames_path = 'depth_frames_eval_1000.npy'\n",
    "            \n",
    "            can_pos_quat = np.load(os.path.join(path, can_pos_quat_path))\n",
    "            depth_frames = np.load(os.path.join(path, depth_frames_path))\n",
    "            eval_data['can_pos_quat'] = can_pos_quat\n",
    "            eval_data['depth_frames'] = depth_frames\n",
    "            print(f\"Loaded eval data with can_pos_quat shape {can_pos_quat.shape} and depth_frames shape {depth_frames.shape}\")\n",
    "            # post process\n",
    "            eval_data['can_pos_quat'] = torch.tensor(eval_data['can_pos_quat'], device=device)\n",
    "            eval_data['depth_frames'] = torch.tensor(eval_data['depth_frames'], device=device)\n",
    "            \n",
    "            eval_data['can_pos_quat'] = eval_data['can_pos_quat'][:, :3]\n",
    "            #print(eval_data['can_pos_quat'][:5])\n",
    "            #eval_data['can_pos_quat'] = (eval_data['can_pos_quat'] - global_mean) / global_std # normalize\n",
    "            #print(eval_data['can_pos_quat'][:5])\n",
    "            # cuda\n",
    "\n",
    "            \n",
    "            model_input = torch.tensor(eval_data['depth_frames'], dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "            \n",
    "            predicted_target = model(model_input)\n",
    "            predicted_target = (predicted_target * global_std) + global_mean\n",
    "            #print(predicted_target[:10])\n",
    "            #print(predicted_target[:10], eval_data['can_pos_quat'][:10])\n",
    "            gt = eval_data['can_pos_quat']\n",
    "            #print(predicted_target.device, gt.device)\n",
    "            error = torch.norm(predicted_target - gt, dim=1)\n",
    "            #error = np.linalg.norm(predicted_target - gt, axis=1)\n",
    "            success_count = torch.sum(error < success_threshold).item()\n",
    "            #success_count = np.sum(error < success_threshold)\n",
    "            success_rate = success_count / num_episodes\n",
    "            #print(f\"Success rate: {success_rate}, Success count: {success_count}, Total episodes: {num_episodes}\")\n",
    "            return {'success_rate': success_rate, 'success_count': success_count, 'total_episodes': num_episodes}\n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "        for i in tqdm.tqdm(range(num_episodes)):\n",
    "            obs, _ = env.reset()\n",
    "            obs = torch.Tensor(obs).to(device)\n",
    "\n",
    "            image_frame = env.envs[0].image_states[viewer_image_key]\n",
    "            if not viewer_image_key.endswith('depth'):\n",
    "                image_frame = np.array(image_frame[::-1, :, :], dtype=np.uint8)  # numpy 배열로 변환\n",
    "            else:\n",
    "                image_frame = np.array(image_frame[::-1, :, :], dtype=np.float32)\n",
    "\n",
    "            model_input = torch.tensor(image_frame, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "            \n",
    "            \n",
    "            predicted_target = model(model_input).cpu().detach().numpy()\n",
    "            predicted_target = (predicted_target * std) + mean\n",
    "        \n",
    "\n",
    "            gt = env.envs[0].sim.data.get_body_xpos('Can_main')\n",
    "            # calculate the error\n",
    "            error = np.linalg.norm(predicted_target - env.envs[0].sim.data.get_body_xpos('Can_main'))\n",
    "            if error < success_threshold:\n",
    "                success_count += 1\n",
    "            if visualize :\n",
    "                print(f\"Predicted: {predicted_target}, GT: {gt}, Error: {error}\")\n",
    "                image_frame = colorize_depth(image_frame)\n",
    "                \n",
    "                can_pos = env.envs[0].sim.data.get_body_xpos('Can_main')  # Assuming the object is called 'Can'\n",
    "                can_quat = env.envs[0].sim.data.get_body_xquat('Can_main')\n",
    "                \n",
    "                error_text = f\"Error: {error.round(4)}\"\n",
    "                # in text, round values\n",
    "                pos_text = f\"GT pos: {can_pos.round(4)}\"\n",
    "                predict_text = f\"Pd pos: {predicted_target.round(4)}\"\n",
    "                cv2.putText(image_frame, error_text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.putText(image_frame, pos_text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "                cv2.putText(image_frame, predict_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "                \n",
    "                frames.append(image_frame)\n",
    "        success_rate = success_count / num_episodes\n",
    "        print(f\"Success rate: {success_rate}, Success count: {success_count}, Total episodes: {num_episodes}\")\n",
    "        if visualize:      \n",
    "            media.show_video(frames, fps=20)\n",
    "        return {'success_rate': success_rate, 'success_count': success_count, 'total_episodes': num_episodes}\n",
    "\n",
    "#evaluate_object_model(model, 100, visualize=False, load_eval_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DepthImageCoordinateRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DepthImageCoordinateRegressor, self).__init__()\n",
    "        # Convolutional layers with larger kernels and strides, no padding\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=8, stride=4, padding=0)  # Output: (64, 63, 63)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0)  # Output: (128, 21, 21)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=0)  # Output: (256, 10, 10)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=0)  # Output: (512, 8, 8)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        # Final pooling layer to reduce spatial dimensions\n",
    "        self.pool = nn.MaxPool2d((8, 8))  # Output: (512, 6, 6)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)  # Output layer for x, y, z (3 real values)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))  # Output: (64, 62, 62)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))  # Output: (128, 30, 30)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))  # Output: (256, 14, 14)\n",
    "        x = self.pool(self.conv4(x))  # Output: (512, 1, 1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        # x = self.pool(x)  # Output: (512, 6, 6)\n",
    "        \n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = x.reshape(-1, 512)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output: (batch_size, 3) - x, y, z\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 5000\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = DepthImageCoordinateRegressor()\n",
    "criterion = nn.MSELoss()#torch.nn.L1Loss() #nn.MSELoss()  # Mean squared error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# add lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1, verbose=True)\n",
    "# Move model to appropriate device\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "import time\n",
    "# Example of training loop\n",
    "def train_model(train_loader, eval_interval=100, eval_load=False, eval_episodes=50, eval_success_threshold=0.01):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_strat_time = time.time()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for images, targets in train_loader:\n",
    "            # Move data to appropriate device\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            #print(f\"target_shape: {targets.shape}\")\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            #print(f\"output_shape: {outputs.shape}\")\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "        #print(f\"Epoch time: {time.time() - epoch_strat_time}, Data per hour: {int(len(train_loader) * batch_size / (time.time() - epoch_strat_time) * 3600)}, lr: {scheduler.get_last_lr()}\")\n",
    "        if epoch % eval_interval == 0 and epoch != 0:\n",
    "            model.eval()\n",
    "            eval_ret = evaluate_object_model(model, num_episodes=eval_episodes, visualize=False, \n",
    "                                             load_eval_data=eval_load)\n",
    "            print(f\"Success rate: {eval_ret['success_rate']}, lr: {scheduler.get_last_lr()}\")\n",
    "            model.train()\n",
    "        \n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5000], Loss: 0.7230866327881813\n",
      "Epoch [2/5000], Loss: 0.39447206631302834\n",
      "Epoch [3/5000], Loss: 0.21959501318633556\n",
      "Epoch [4/5000], Loss: 0.15059364400804043\n",
      "Epoch [5/5000], Loss: 0.11421135254204273\n",
      "Epoch [6/5000], Loss: 0.09634633548557758\n",
      "Epoch [7/5000], Loss: 0.08914897218346596\n",
      "Epoch [8/5000], Loss: 0.08138623926788568\n",
      "Epoch [9/5000], Loss: 0.07331704627722502\n",
      "Epoch [10/5000], Loss: 0.06802753452211618\n",
      "Epoch [11/5000], Loss: 0.06874298630282283\n",
      "Epoch [12/5000], Loss: 0.06625239364802837\n",
      "Epoch [13/5000], Loss: 0.06414586957544088\n",
      "Epoch [14/5000], Loss: 0.06559728737920523\n",
      "Epoch [15/5000], Loss: 0.06464461889117956\n",
      "Epoch [16/5000], Loss: 0.061561651062220335\n",
      "Epoch [17/5000], Loss: 0.06014058040454984\n",
      "Epoch [18/5000], Loss: 0.06025381200015545\n",
      "Epoch [19/5000], Loss: 0.060132110957056284\n",
      "Epoch [20/5000], Loss: 0.059110716450959444\n",
      "Epoch [21/5000], Loss: 0.05629888642579317\n",
      "Epoch [22/5000], Loss: 0.05953348195180297\n",
      "Epoch [23/5000], Loss: 0.06012192368507385\n",
      "Epoch [24/5000], Loss: 0.05905852746218443\n",
      "Epoch [25/5000], Loss: 0.06351922545582056\n",
      "Epoch [26/5000], Loss: 0.06620633183047175\n",
      "Epoch [27/5000], Loss: 0.06053785793483257\n",
      "Epoch [28/5000], Loss: 0.058717877604067326\n",
      "Epoch [29/5000], Loss: 0.05705641396343708\n",
      "Epoch [30/5000], Loss: 0.05711033986881375\n",
      "Epoch [31/5000], Loss: 0.05693748267367482\n",
      "Epoch [32/5000], Loss: 0.05610032193362713\n",
      "Epoch [33/5000], Loss: 0.05610395967960358\n",
      "Epoch [34/5000], Loss: 0.05627974076196551\n",
      "Epoch [35/5000], Loss: 0.05800120020285249\n",
      "Epoch [36/5000], Loss: 0.0566957644186914\n",
      "Epoch [37/5000], Loss: 0.058239377569407225\n",
      "Epoch [38/5000], Loss: 0.05871116882190108\n",
      "Epoch [39/5000], Loss: 0.05759981879964471\n",
      "Epoch [40/5000], Loss: 0.0567715922370553\n",
      "Epoch [41/5000], Loss: 0.05698115611448884\n",
      "Epoch [42/5000], Loss: 0.0562246642075479\n",
      "Epoch [43/5000], Loss: 0.05521912546828389\n",
      "Epoch [44/5000], Loss: 0.05564644327387214\n",
      "Epoch [45/5000], Loss: 0.05719725275412202\n",
      "Epoch [46/5000], Loss: 0.05672337720170617\n",
      "Epoch [47/5000], Loss: 0.05577453225851059\n",
      "Epoch [48/5000], Loss: 0.05538304056972265\n",
      "Epoch [49/5000], Loss: 0.05550866760313511\n",
      "Epoch [50/5000], Loss: 0.05619221879169345\n",
      "Epoch [51/5000], Loss: 0.055130959022790194\n",
      "Loaded eval data with can_pos_quat shape (1000, 7) and depth_frames shape (1000, 256, 256, 1)\n",
      "Success rate: 0.0, lr: [0.0001]\n",
      "Epoch [52/5000], Loss: 0.05502466205507517\n",
      "Epoch [53/5000], Loss: 0.05511529045179486\n",
      "Epoch [54/5000], Loss: 0.05487424274906516\n",
      "Epoch [55/5000], Loss: 0.05502445111051202\n",
      "Epoch [56/5000], Loss: 0.05474840244278312\n",
      "Epoch [57/5000], Loss: 0.05471108062192798\n",
      "Epoch [58/5000], Loss: 0.054686909541487694\n",
      "Epoch [59/5000], Loss: 0.054808313492685556\n",
      "Epoch [60/5000], Loss: 0.054817513562738895\n",
      "Epoch [61/5000], Loss: 0.05486582266166806\n",
      "Epoch [62/5000], Loss: 0.05482741165906191\n",
      "Epoch [63/5000], Loss: 0.05480559542775154\n",
      "Epoch [64/5000], Loss: 0.05495527572929859\n",
      "Epoch [65/5000], Loss: 0.054562830831855536\n",
      "Epoch [66/5000], Loss: 0.05502059916034341\n",
      "Epoch [67/5000], Loss: 0.05450623435899615\n",
      "Epoch [68/5000], Loss: 0.05478479107841849\n",
      "Epoch [69/5000], Loss: 0.05468272464349866\n",
      "Epoch [70/5000], Loss: 0.05458643473684788\n",
      "Epoch [71/5000], Loss: 0.05490518268197775\n",
      "Epoch [72/5000], Loss: 0.05520997289568186\n",
      "Epoch [73/5000], Loss: 0.05450876709073782\n",
      "Epoch [74/5000], Loss: 0.0548618333414197\n",
      "Epoch [75/5000], Loss: 0.05473410990089178\n",
      "Epoch [76/5000], Loss: 0.055237310472875834\n",
      "Epoch [77/5000], Loss: 0.05487412726506591\n",
      "Epoch [78/5000], Loss: 0.054744564928114414\n",
      "Epoch [79/5000], Loss: 0.05453632399439812\n",
      "Epoch [80/5000], Loss: 0.05482570920139551\n",
      "Epoch [81/5000], Loss: 0.054746265057474375\n",
      "Epoch [82/5000], Loss: 0.054684133268892765\n",
      "Epoch [83/5000], Loss: 0.05462996056303382\n",
      "Epoch [84/5000], Loss: 0.05504629760980606\n",
      "Epoch [85/5000], Loss: 0.05487501807510853\n",
      "Epoch [86/5000], Loss: 0.054810527712106705\n",
      "Epoch [87/5000], Loss: 0.05461755208671093\n",
      "Epoch [88/5000], Loss: 0.05528884567320347\n",
      "Epoch [89/5000], Loss: 0.054620280396193266\n",
      "Epoch [90/5000], Loss: 0.05490025505423546\n",
      "Epoch [91/5000], Loss: 0.054886206053197384\n",
      "Epoch [92/5000], Loss: 0.05493065109476447\n",
      "Epoch [93/5000], Loss: 0.05481355870142579\n",
      "Epoch [94/5000], Loss: 0.0549208247102797\n",
      "Epoch [95/5000], Loss: 0.05455230036750436\n",
      "Epoch [96/5000], Loss: 0.05476340651512146\n",
      "Epoch [97/5000], Loss: 0.05487333331257105\n",
      "Epoch [98/5000], Loss: 0.054569026455283165\n",
      "Epoch [99/5000], Loss: 0.05502435006201267\n",
      "Epoch [100/5000], Loss: 0.054908736143261194\n",
      "Epoch [101/5000], Loss: 0.05455285497009754\n",
      "Loaded eval data with can_pos_quat shape (1000, 7) and depth_frames shape (1000, 256, 256, 1)\n",
      "Success rate: 0.003, lr: [1e-05]\n",
      "Epoch [102/5000], Loss: 0.05459092231467366\n",
      "Epoch [103/5000], Loss: 0.05456340592354536\n",
      "Epoch [104/5000], Loss: 0.05481294868513942\n",
      "Epoch [105/5000], Loss: 0.05461289128288627\n",
      "Epoch [106/5000], Loss: 0.05446162214502692\n",
      "Epoch [107/5000], Loss: 0.054789344780147076\n",
      "Epoch [108/5000], Loss: 0.054539842531085014\n",
      "Epoch [109/5000], Loss: 0.054746706038713455\n",
      "Epoch [110/5000], Loss: 0.05462351534515619\n",
      "Epoch [111/5000], Loss: 0.05476849200204015\n",
      "Epoch [112/5000], Loss: 0.054763542022556067\n",
      "Epoch [113/5000], Loss: 0.054664854891598225\n",
      "Epoch [114/5000], Loss: 0.05460241995751858\n",
      "Epoch [115/5000], Loss: 0.05467277625575662\n",
      "Epoch [116/5000], Loss: 0.054719646461308\n",
      "Epoch [117/5000], Loss: 0.054514821618795395\n",
      "Epoch [118/5000], Loss: 0.05465765576809645\n",
      "Epoch [119/5000], Loss: 0.05467901658266783\n",
      "Epoch [120/5000], Loss: 0.05453817406669259\n",
      "Epoch [121/5000], Loss: 0.05480632930994034\n",
      "Epoch [122/5000], Loss: 0.054622740019112825\n",
      "Epoch [123/5000], Loss: 0.05449043167755008\n",
      "Epoch [124/5000], Loss: 0.05450814124196768\n",
      "Epoch [125/5000], Loss: 0.05455305567011237\n",
      "Epoch [126/5000], Loss: 0.05463310796767473\n",
      "Epoch [127/5000], Loss: 0.05425240425392985\n",
      "Epoch [128/5000], Loss: 0.05431020585820079\n",
      "Epoch [129/5000], Loss: 0.054610445629805326\n",
      "Epoch [130/5000], Loss: 0.05452960776165128\n",
      "Epoch [131/5000], Loss: 0.05475599318742752\n",
      "Epoch [132/5000], Loss: 0.05463695339858532\n",
      "Epoch [133/5000], Loss: 0.05473417369648814\n",
      "Epoch [134/5000], Loss: 0.05465117562562227\n",
      "Epoch [135/5000], Loss: 0.05446709133684635\n",
      "Epoch [136/5000], Loss: 0.0544915571808815\n",
      "Epoch [137/5000], Loss: 0.054286804515868425\n",
      "Epoch [138/5000], Loss: 0.054587187711149454\n",
      "Epoch [139/5000], Loss: 0.05465267365798354\n",
      "Epoch [140/5000], Loss: 0.054672094993293285\n",
      "Epoch [141/5000], Loss: 0.054863191209733486\n",
      "Epoch [142/5000], Loss: 0.0540120170917362\n",
      "Epoch [143/5000], Loss: 0.054563074838370085\n",
      "Epoch [144/5000], Loss: 0.054794563446193933\n",
      "Epoch [145/5000], Loss: 0.05445430148392916\n",
      "Epoch [146/5000], Loss: 0.05437817610800266\n",
      "Epoch [147/5000], Loss: 0.05455275345593691\n",
      "Epoch [148/5000], Loss: 0.05480202799662948\n",
      "Epoch [149/5000], Loss: 0.05471467366442084\n",
      "Epoch [150/5000], Loss: 0.05486437072977424\n",
      "Epoch [151/5000], Loss: 0.054806086234748363\n",
      "Loaded eval data with can_pos_quat shape (1000, 7) and depth_frames shape (1000, 256, 256, 1)\n",
      "Success rate: 0.737, lr: [1.0000000000000002e-06]\n",
      "Epoch [152/5000], Loss: 0.054280889220535755\n",
      "Epoch [153/5000], Loss: 0.05431623896583915\n",
      "Epoch [154/5000], Loss: 0.05424938956275582\n",
      "Epoch [155/5000], Loss: 0.054621192160993814\n",
      "Epoch [156/5000], Loss: 0.05474757822230458\n",
      "Epoch [157/5000], Loss: 0.05433382233604789\n",
      "Epoch [158/5000], Loss: 0.054290801752358675\n",
      "Epoch [159/5000], Loss: 0.05455825664103031\n",
      "Epoch [160/5000], Loss: 0.05473872320726514\n",
      "Epoch [161/5000], Loss: 0.05456199822947383\n",
      "Epoch [162/5000], Loss: 0.0544142653234303\n",
      "Epoch [163/5000], Loss: 0.054269037675112486\n",
      "Epoch [164/5000], Loss: 0.05472253914922476\n",
      "Epoch [165/5000], Loss: 0.05432078521698713\n",
      "Epoch [166/5000], Loss: 0.054763773921877146\n",
      "Epoch [167/5000], Loss: 0.054281122982501984\n",
      "Epoch [168/5000], Loss: 0.05459827370941639\n",
      "Epoch [169/5000], Loss: 0.05439368262887001\n",
      "Epoch [170/5000], Loss: 0.054617045912891626\n",
      "Epoch [171/5000], Loss: 0.05454159202054143\n",
      "Epoch [172/5000], Loss: 0.054603131487965584\n",
      "Epoch [173/5000], Loss: 0.054543721955269575\n",
      "Epoch [174/5000], Loss: 0.054510371293872595\n",
      "Epoch [175/5000], Loss: 0.05470263212919235\n",
      "Epoch [176/5000], Loss: 0.054468799382448196\n",
      "Epoch [177/5000], Loss: 0.054524962324649096\n",
      "Epoch [178/5000], Loss: 0.05434603849425912\n",
      "Epoch [179/5000], Loss: 0.05472002038732171\n",
      "Epoch [180/5000], Loss: 0.05432058358564973\n",
      "Epoch [181/5000], Loss: 0.05457596434280276\n",
      "Epoch [182/5000], Loss: 0.054700399748981\n",
      "Epoch [183/5000], Loss: 0.05442919582128525\n",
      "Epoch [184/5000], Loss: 0.05431064125150442\n",
      "Epoch [185/5000], Loss: 0.05454910034313798\n",
      "Epoch [186/5000], Loss: 0.05455767922103405\n",
      "Epoch [187/5000], Loss: 0.05454206885769963\n",
      "Epoch [188/5000], Loss: 0.054469316732138395\n",
      "Epoch [189/5000], Loss: 0.05509548494592309\n",
      "Epoch [190/5000], Loss: 0.05449494952335954\n",
      "Epoch [191/5000], Loss: 0.05432829214259982\n",
      "Epoch [192/5000], Loss: 0.05472729168832302\n",
      "Epoch [193/5000], Loss: 0.054844729136675596\n",
      "Epoch [194/5000], Loss: 0.05490403063595295\n",
      "Epoch [195/5000], Loss: 0.054827443324029446\n",
      "Epoch [196/5000], Loss: 0.05459564831107855\n",
      "Epoch [197/5000], Loss: 0.05453875195235014\n",
      "Epoch [198/5000], Loss: 0.054771787486970425\n",
      "Epoch [199/5000], Loss: 0.05440292600542307\n",
      "Epoch [200/5000], Loss: 0.05475505022332072\n",
      "Epoch [201/5000], Loss: 0.05462912796065211\n",
      "Loaded eval data with can_pos_quat shape (1000, 7) and depth_frames shape (1000, 256, 256, 1)\n",
      "Success rate: 0.765, lr: [1.0000000000000002e-07]\n",
      "Epoch [202/5000], Loss: 0.05477530136704445\n",
      "Epoch [203/5000], Loss: 0.05458696885034442\n",
      "Epoch [204/5000], Loss: 0.054700799752026796\n",
      "Epoch [205/5000], Loss: 0.05455334763973951\n",
      "Epoch [206/5000], Loss: 0.054480066522955894\n",
      "Epoch [207/5000], Loss: 0.05463605048134923\n",
      "Epoch [208/5000], Loss: 0.05463050911203027\n",
      "Epoch [209/5000], Loss: 0.05475027672946453\n",
      "Epoch [210/5000], Loss: 0.05486689042299986\n",
      "Epoch [211/5000], Loss: 0.05446088733151555\n",
      "Epoch [212/5000], Loss: 0.054907061625272036\n",
      "Epoch [213/5000], Loss: 0.054406207986176014\n",
      "Epoch [214/5000], Loss: 0.05441316124051809\n",
      "Epoch [215/5000], Loss: 0.05441363016143441\n",
      "Epoch [216/5000], Loss: 0.054701732005923986\n",
      "Epoch [217/5000], Loss: 0.05427173059433699\n",
      "Epoch [218/5000], Loss: 0.05465742480009794\n",
      "Epoch [219/5000], Loss: 0.05445462744683027\n",
      "Epoch [220/5000], Loss: 0.0545623772777617\n",
      "Epoch [221/5000], Loss: 0.05435315100476146\n",
      "Epoch [222/5000], Loss: 0.054685141891241074\n",
      "Epoch [223/5000], Loss: 0.054389288648962975\n",
      "Epoch [224/5000], Loss: 0.054617546033114195\n",
      "Epoch [225/5000], Loss: 0.054565617348998785\n",
      "Epoch [226/5000], Loss: 0.05477452138438821\n",
      "Epoch [227/5000], Loss: 0.05468691745772958\n",
      "Epoch [228/5000], Loss: 0.05501315835863352\n",
      "Epoch [229/5000], Loss: 0.054722332395613194\n",
      "Epoch [230/5000], Loss: 0.054452605079859495\n",
      "Epoch [231/5000], Loss: 0.054727704264223576\n",
      "Epoch [232/5000], Loss: 0.05471681384369731\n",
      "Epoch [233/5000], Loss: 0.05459822807461023\n",
      "Epoch [234/5000], Loss: 0.05459248647093773\n",
      "Epoch [235/5000], Loss: 0.05462840665131807\n",
      "Epoch [236/5000], Loss: 0.054895839653909206\n",
      "Epoch [237/5000], Loss: 0.05455279629677534\n",
      "Epoch [238/5000], Loss: 0.05440578144043684\n",
      "Epoch [239/5000], Loss: 0.05467612436041236\n",
      "Epoch [240/5000], Loss: 0.05463101202622056\n",
      "Epoch [241/5000], Loss: 0.054738677106797695\n",
      "Epoch [242/5000], Loss: 0.05490464670583606\n",
      "Epoch [243/5000], Loss: 0.05465141078457236\n",
      "Epoch [244/5000], Loss: 0.05463070189580321\n",
      "Epoch [245/5000], Loss: 0.05447749234735966\n",
      "Epoch [246/5000], Loss: 0.05447761435061693\n",
      "Epoch [247/5000], Loss: 0.054441276006400585\n",
      "Epoch [248/5000], Loss: 0.05442176666110754\n",
      "Epoch [249/5000], Loss: 0.05462537566199899\n",
      "Epoch [250/5000], Loss: 0.05451571196317673\n",
      "Epoch [251/5000], Loss: 0.05467014340683818\n",
      "Loaded eval data with can_pos_quat shape (1000, 7) and depth_frames shape (1000, 256, 256, 1)\n",
      "Success rate: 0.765, lr: [1.0000000000000004e-08]\n",
      "Epoch [252/5000], Loss: 0.05476120999082923\n",
      "Epoch [253/5000], Loss: 0.05466054705902934\n",
      "Epoch [254/5000], Loss: 0.054751360323280096\n",
      "Epoch [255/5000], Loss: 0.054591397754848\n",
      "Epoch [256/5000], Loss: 0.05462196795269847\n",
      "Epoch [257/5000], Loss: 0.054503587540239096\n",
      "Epoch [258/5000], Loss: 0.05437758192420006\n",
      "Epoch [259/5000], Loss: 0.054219591896981\n",
      "Epoch [260/5000], Loss: 0.054565899074077606\n",
      "Epoch [261/5000], Loss: 0.05454884236678481\n",
      "Epoch [262/5000], Loss: 0.05436151986941695\n",
      "Epoch [263/5000], Loss: 0.05454463930800557\n",
      "Epoch [264/5000], Loss: 0.054553006775677204\n",
      "Epoch [265/5000], Loss: 0.05456841131672263\n",
      "Epoch [266/5000], Loss: 0.05429697362706065\n",
      "Epoch [267/5000], Loss: 0.05442301230505109\n",
      "Epoch [268/5000], Loss: 0.05466803489252925\n",
      "Epoch [269/5000], Loss: 0.054580673575401306\n",
      "Epoch [270/5000], Loss: 0.05470382422208786\n",
      "Epoch [271/5000], Loss: 0.054521006532013416\n",
      "Epoch [272/5000], Loss: 0.0546107217669487\n",
      "Epoch [273/5000], Loss: 0.054538579657673836\n",
      "Epoch [274/5000], Loss: 0.054412330500781536\n",
      "Epoch [275/5000], Loss: 0.05439061811193824\n",
      "Epoch [276/5000], Loss: 0.05487920576706529\n",
      "Epoch [277/5000], Loss: 0.05470250407233834\n",
      "Epoch [278/5000], Loss: 0.05426153866574168\n",
      "Epoch [279/5000], Loss: 0.05436342256143689\n",
      "Epoch [280/5000], Loss: 0.05481939250603318\n",
      "Epoch [281/5000], Loss: 0.05472889868542552\n",
      "Epoch [282/5000], Loss: 0.05450651561841369\n",
      "Epoch [283/5000], Loss: 0.05464297439903021\n",
      "Epoch [284/5000], Loss: 0.05488490452989936\n",
      "Epoch [285/5000], Loss: 0.05442785331979394\n",
      "Epoch [286/5000], Loss: 0.054704141803085804\n",
      "Epoch [287/5000], Loss: 0.054397772531956434\n",
      "Epoch [288/5000], Loss: 0.05444675264880061\n",
      "Epoch [289/5000], Loss: 0.05454079573974013\n",
      "Epoch [290/5000], Loss: 0.05424927221611142\n",
      "Epoch [291/5000], Loss: 0.05475078569725156\n",
      "Epoch [292/5000], Loss: 0.05470308568328619\n",
      "Epoch [293/5000], Loss: 0.05470914766192436\n",
      "Epoch [294/5000], Loss: 0.05447235330939293\n",
      "Epoch [295/5000], Loss: 0.0543625233694911\n",
      "Epoch [296/5000], Loss: 0.05515300203114748\n",
      "Epoch [297/5000], Loss: 0.054732533637434244\n",
      "Epoch [298/5000], Loss: 0.054655204992741346\n",
      "Epoch [299/5000], Loss: 0.05463978508487344\n",
      "Epoch [300/5000], Loss: 0.054401260800659657\n",
      "Epoch [301/5000], Loss: 0.054611061699688435\n",
      "Loaded eval data with can_pos_quat shape (1000, 7) and depth_frames shape (1000, 256, 256, 1)\n",
      "Success rate: 0.764, lr: [1.0000000000000005e-09]\n",
      "Epoch [302/5000], Loss: 0.05456722993403673\n",
      "Epoch [303/5000], Loss: 0.05450378870591521\n",
      "Epoch [304/5000], Loss: 0.05441305646672845\n",
      "Epoch [305/5000], Loss: 0.0544830528087914\n",
      "Epoch [306/5000], Loss: 0.05468695517629385\n",
      "Epoch [307/5000], Loss: 0.054391863755881786\n",
      "Epoch [308/5000], Loss: 0.054480268619954586\n",
      "Epoch [309/5000], Loss: 0.05431916704401374\n",
      "Epoch [310/5000], Loss: 0.05439067259430885\n",
      "Epoch [311/5000], Loss: 0.054536746349185705\n",
      "Epoch [312/5000], Loss: 0.05467693042010069\n",
      "Epoch [313/5000], Loss: 0.05462075071409345\n",
      "Epoch [314/5000], Loss: 0.05476180976256728\n",
      "Epoch [315/5000], Loss: 0.0548501149751246\n",
      "Epoch [316/5000], Loss: 0.054516575299203396\n",
      "Epoch [317/5000], Loss: 0.05416910629719496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_load\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_success_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 81\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, eval_interval, eval_load, eval_episodes, eval_success_threshold)\u001b[0m\n\u001b[1;32m     78\u001b[0m epoch_strat_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     80\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Move data to appropriate device\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     images, targets \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m#print(f\"target_shape: {targets.shape}\")\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rs4/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/rs4/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(train_loader, eval_load=True, eval_episodes=1000, eval_interval=50, \n",
    "            eval_success_threshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), f'checkpoints/model_1000.pth')\n",
    "print(\"Model saved to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load model\n",
    "model = DepthImageCoordinateRegressor()\n",
    "model.load_state_dict(torch.load(f'checkpoints/model_1000.pth'))\n",
    "model.eval()\n",
    "print(\"Model loaded from model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
