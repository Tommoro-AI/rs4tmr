{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 samples from can_pos_quat_1.npy\n",
      "Loaded 1000 samples from can_pos_quat_2.npy\n",
      "Concatenated can_pos_quat with shape (2000, 7)\n",
      "Concatenated depth_frames with shape (2000, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path = 'data'\n",
    "file_prefix_list = ['depth_frames', 'can_pos_quat']\n",
    "file_suffix_range = range(1, 3)\n",
    "\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data = {}\n",
    "data['depth_frames'] = []\n",
    "data['can_pos_quat'] = []\n",
    "# interate over all files\n",
    "for idx in file_suffix_range:\n",
    "    can_pos_quat_path = f\"can_pos_quat_{idx}.npy\"\n",
    "    depth_frames_path = f\"depth_frames_{idx}.npy\"\n",
    "    \n",
    "    can_pos_quat = np.load(os.path.join(path, can_pos_quat_path))\n",
    "    depth_frames = np.load(os.path.join(path, depth_frames_path))\n",
    "    print(f\"Loaded {can_pos_quat.shape[0]} samples from {can_pos_quat_path}\")\n",
    "    # concat\n",
    "    data['can_pos_quat'].append(can_pos_quat)\n",
    "    data['depth_frames'].append(depth_frames)\n",
    "    \n",
    "\n",
    "# concatenate all data\n",
    "data['can_pos_quat'] = np.concatenate(data['can_pos_quat'], axis=0)\n",
    "data['depth_frames'] = np.concatenate(data['depth_frames'], axis=0)\n",
    "\n",
    "print(f\"Concatenated can_pos_quat with shape {data['can_pos_quat'].shape}\")\n",
    "print(f\"Concatenated depth_frames with shape {data['depth_frames'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting the quaternion to only the first 3 elements. New shape: (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Post process\n",
    "\n",
    "# Cutting the quaternion to only the first 3 elements\n",
    "data['can_pos_quat'] = data['can_pos_quat'][:, :3]\n",
    "print(f\"Cutting the quaternion to only the first 3 elements. New shape: {data['can_pos_quat'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize can_pos_quat\n",
    "mean = data['can_pos_quat'].mean(axis=0)\n",
    "std = data['can_pos_quat'].std(axis=0)\n",
    "data['can_pos_quat'] = (data['can_pos_quat'] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Example usage\\nfor batch_idx, (depth_batch, target_batch) in enumerate(train_loader):\\n    print(f\"Batch {batch_idx+1} - Depth batch shape: {depth_batch.shape}, Target batch shape: {target_batch.shape}\")\\n    # Depth batch shape: [batch_size, 1, 256, 256]\\n    # Target batch shape: [batch_size, 3] (assuming \\'can_pos_quat\\' has 3 values to predict)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class DepthCanDataset(Dataset):\n",
    "    def __init__(self, depth_frames, can_pos_quat):\n",
    "        # Depth frames and corresponding position/quaternion data\n",
    "        # Permute depth frames from (batch, 256, 256, 1) to (batch, 1, 256, 256)\n",
    "        self.depth_frames = torch.tensor(depth_frames, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        self.can_pos_quat = torch.tensor(can_pos_quat, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.can_pos_quat)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        depth_frame = self.depth_frames[idx]\n",
    "        can_pos_quat = self.can_pos_quat[idx]\n",
    "        \n",
    "        return depth_frame, can_pos_quat\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = DepthCanDataset(data['depth_frames'], data['can_pos_quat'][:, :3])\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "'''\n",
    "# Example usage\n",
    "for batch_idx, (depth_batch, target_batch) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx+1} - Depth batch shape: {depth_batch.shape}, Target batch shape: {target_batch.shape}\")\n",
    "    # Depth batch shape: [batch_size, 1, 256, 256]\n",
    "    # Target batch shape: [batch_size, 3] (assuming 'can_pos_quat' has 3 values to predict)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DepthImageRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DepthImageRegressor, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Max Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 512)  # Adjust based on the output size after conv layers\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 3)  # Output layer for 3 real numbers\n",
    "        \n",
    "        # Activation and normalization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU activation and MaxPooling\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.pool(self.relu(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the tensor\n",
    "        x = x.reshape(-1, 256 * 16 * 16)  # Use reshape instead of view\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer (no activation, because we are predicting real numbers)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Output constrained to -1 to 1\n",
    "        return torch.tanh(x)\n",
    "\n",
    "# Hyperparameters\n",
    "#batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = DepthImageRegressor()\n",
    "criterion = nn.MSELoss()  # Mean squared error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Example of training loop\n",
    "def train_model(train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, targets in train_loader:\n",
    "            # Move data to appropriate device\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7505735829472542\n",
      "Epoch [2/5], Loss: 0.6887340135872364\n",
      "Epoch [3/5], Loss: 0.6856138110160828\n",
      "Epoch [4/5], Loss: 0.6829417012631893\n",
      "Epoch [5/5], Loss: 0.6852454319596291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "train_model(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /research/rs4tmr/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "from my_utils import get_current_time\n",
    "import time\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), f'checkpoints/model_2k_5_norm.pth')\n",
    "print(\"Model saved to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load model\n",
    "model = DepthImageRegressor()\n",
    "model.load_state_dict(torch.load(f'checkpoints/model_norm.pth'))\n",
    "model.eval()\n",
    "print(\"Model loaded from model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### controller_config: OSC_POSITION ###\n",
      "#### J PickPlace ####\n",
      "fix_object:False\n",
      "start with grasp lock: True\n",
      "control_freq: 20\n",
      "ignore_done: False\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "min() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * ()\n * (Tensor other)\n * (int dim, bool keepdim = False)\n      didn't match because some of the keywords were incorrect: out, axis\n * (name dim, bool keepdim = False)\n      didn't match because some of the keywords were incorrect: out, axis\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 88\u001b[0m\n\u001b[1;32m     84\u001b[0m     image_frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image_frame[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     86\u001b[0m model_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(image_frame, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput frame shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, min: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmin(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmax(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m predicted_target \u001b[38;5;241m=\u001b[39m model(model_input)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     92\u001b[0m predicted_target \u001b[38;5;241m=\u001b[39m (predicted_target \u001b[38;5;241m*\u001b[39m std) \u001b[38;5;241m+\u001b[39m mean\n",
      "File \u001b[0;32m~/anaconda3/envs/rs4tmr/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3042\u001b[0m, in \u001b[0;36mmin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2925\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_min_dispatcher)\n\u001b[1;32m   2926\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2927\u001b[0m         where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2929\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[1;32m   2930\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3040\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[1;32m   3041\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3042\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3043\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rs4tmr/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:84\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: min() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * ()\n * (Tensor other)\n * (int dim, bool keepdim = False)\n      didn't match because some of the keywords were incorrect: out, axis\n * (name dim, bool keepdim = False)\n      didn't match because some of the keywords were incorrect: out, axis\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "# 필요한 클래스 및 함수 임포트\n",
    "from cleanrl.cleanrl.ppo_continuous_action import Agent, Args, ppo_make_env\n",
    "import cv2\n",
    "from robosuite.utils.camera_utils import CameraMover\n",
    "\n",
    "visualize = True\n",
    "frames = []\n",
    "\n",
    "# Argument 설정\n",
    "task_id = 'pickplace'\n",
    "seed = 0\n",
    "gamma = 0.99\n",
    "num_episodes = 1\n",
    "render_camera = ['birdview']#,'agentview'] #('frontview', 'birdview', 'agentview', 'robot0_robotview', 'robot0_eye_in_hand')\n",
    "camera_names = render_camera\n",
    "\n",
    "\n",
    "# 환경 생성\n",
    "env = gym.vector.SyncVectorEnv(\n",
    "    [ppo_make_env(\n",
    "        task_id=task_id, \n",
    "        reward_shaping=True,\n",
    "        idx=0, \n",
    "        control_mode=\"OSC_POSITION\",\n",
    "        capture_video=False, \n",
    "        run_name=\"eval\", \n",
    "        gamma= gamma, \n",
    "        active_rewards=\"r\",\n",
    "        active_image=True, \n",
    "        fix_object=False,\n",
    "        wandb_enabled=False,\n",
    "        verbose=False,\n",
    "        control_freq=20,\n",
    "        render_camera=render_camera,\n",
    "        camera_names=camera_names,\n",
    "\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def colorize_depth(frame):\n",
    "    # Assuming the depth image is in float32 and contains values representing distances.\n",
    "    # Normalize the depth image to 0-255 for visualization\n",
    "    min_depth = np.min(frame)\n",
    "    max_depth = np.max(frame)\n",
    "    normalized_depth = 255 * (frame - min_depth) / (max_depth - min_depth)\n",
    "    normalized_depth = normalized_depth.astype(np.uint8)\n",
    "\n",
    "    # Apply a colormap for better visualization (COLORMAP_JET is commonly used)\n",
    "    colorized_depth = cv2.applyColorMap(normalized_depth, cv2.COLORMAP_JET)\n",
    "    return colorized_depth\n",
    "\n",
    "\n",
    "# 디바이스 설정 (cuda가 가능하면 cuda 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    print(\"Using CUDA\")\n",
    "else :\n",
    "    assert device == torch.device(\"cpu\")\n",
    "\n",
    "# 평가 수행\n",
    "total_rewards = []\n",
    "viewer_image_key = 'birdview'+'_depth'\n",
    "\n",
    "# generate samples\n",
    "num_samples = 10\n",
    "\n",
    "model.eval()\n",
    "for i in range(num_samples):\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    image_frame = env.envs[0].image_states[viewer_image_key]\n",
    "    if not viewer_image_key.endswith('depth'):\n",
    "        image_frame = np.array(image_frame[::-1, :, :], dtype=np.uint8)  # numpy 배열로 변환\n",
    "    else:\n",
    "        image_frame = np.array(image_frame[::-1, :, :], dtype=np.float32)\n",
    "\n",
    "    model_input = torch.tensor(image_frame, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "    \n",
    "    predicted_target = model(model_input).cpu().detach().numpy()\n",
    "    predicted_target = (predicted_target * std) + mean\n",
    "\n",
    "    print(f\"Predicted target: {predicted_target}\")\n",
    "    print(f\"GT target: {env.envs[0].sim.data.get_body_xpos('Can_main')}\")\n",
    "\n",
    "    image_frame = colorize_depth(image_frame)\n",
    "    \n",
    "    can_pos = env.envs[0].sim.data.get_body_xpos('Can_main')  # Assuming the object is called 'Can'\n",
    "    can_quat = env.envs[0].sim.data.get_body_xquat('Can_main')\n",
    "    pos_text = f\"Pos: {can_pos}\"\n",
    "    quat_text = f\"Quat: {can_quat}\"\n",
    "    cv2.putText(image_frame, pos_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.2, (255, 255, 255), 1)\n",
    "    cv2.putText(image_frame, quat_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.2, (255, 255, 255), 1)\n",
    "   \n",
    "    \n",
    "    frames.append(image_frame)\n",
    "\n",
    "media.show_video(frames, fps=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs4tmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
