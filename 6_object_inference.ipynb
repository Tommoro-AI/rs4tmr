{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 samples from can_pos_quat_1.npy\n",
      "Loaded 1000 samples from can_pos_quat_2.npy\n",
      "Concatenated can_pos_quat with shape (2000, 7)\n",
      "Concatenated depth_frames with shape (2000, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path = 'data'\n",
    "file_prefix_list = ['depth_frames', 'can_pos_quat']\n",
    "file_suffix_range = range(1, 3)\n",
    "\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data = {}\n",
    "data['depth_frames'] = []\n",
    "data['can_pos_quat'] = []\n",
    "# interate over all files\n",
    "for idx in file_suffix_range:\n",
    "    can_pos_quat_path = f\"can_pos_quat_{idx}.npy\"\n",
    "    depth_frames_path = f\"depth_frames_{idx}.npy\"\n",
    "    \n",
    "    can_pos_quat = np.load(os.path.join(path, can_pos_quat_path))\n",
    "    depth_frames = np.load(os.path.join(path, depth_frames_path))\n",
    "    print(f\"Loaded {can_pos_quat.shape[0]} samples from {can_pos_quat_path}\")\n",
    "    # concat\n",
    "    data['can_pos_quat'].append(can_pos_quat)\n",
    "    data['depth_frames'].append(depth_frames)\n",
    "    \n",
    "\n",
    "# concatenate all data\n",
    "data['can_pos_quat'] = np.concatenate(data['can_pos_quat'], axis=0)\n",
    "data['depth_frames'] = np.concatenate(data['depth_frames'], axis=0)\n",
    "\n",
    "print(f\"Concatenated can_pos_quat with shape {data['can_pos_quat'].shape}\")\n",
    "print(f\"Concatenated depth_frames with shape {data['depth_frames'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting the quaternion to only the first 3 elements. New shape: (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Post process\n",
    "\n",
    "# Cutting the quaternion to only the first 3 elements\n",
    "data['can_pos_quat'] = data['can_pos_quat'][:, :3]\n",
    "print(f\"Cutting the quaternion to only the first 3 elements. New shape: {data['can_pos_quat'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized can_pos_quat with mean [ 0.099548   -0.25041619  0.86      ] and std [7.26558138e-02 1.02460468e-01 3.35287353e-14]\n"
     ]
    }
   ],
   "source": [
    "# Normalize can_pos_quat\n",
    "mean = data['can_pos_quat'].mean(axis=0)\n",
    "std = data['can_pos_quat'].std(axis=0)\n",
    "data['can_pos_quat'] = (data['can_pos_quat'] - mean) / std\n",
    "print(f\"Normalized can_pos_quat with mean {mean} and std {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Example usage\\nfor batch_idx, (depth_batch, target_batch) in enumerate(train_loader):\\n    print(f\"Batch {batch_idx+1} - Depth batch shape: {depth_batch.shape}, Target batch shape: {target_batch.shape}\")\\n    # Depth batch shape: [batch_size, 1, 256, 256]\\n    # Target batch shape: [batch_size, 3] (assuming \\'can_pos_quat\\' has 3 values to predict)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class DepthCanDataset(Dataset):\n",
    "    def __init__(self, depth_frames, can_pos_quat):\n",
    "        # Depth frames and corresponding position/quaternion data\n",
    "        # Permute depth frames from (batch, 256, 256, 1) to (batch, 1, 256, 256)\n",
    "        self.depth_frames = torch.tensor(depth_frames, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        self.can_pos_quat = torch.tensor(can_pos_quat, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.can_pos_quat)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        depth_frame = self.depth_frames[idx]\n",
    "        can_pos_quat = self.can_pos_quat[idx]\n",
    "        \n",
    "        return depth_frame, can_pos_quat\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = DepthCanDataset(data['depth_frames'], data['can_pos_quat'][:, :3])\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "'''\n",
    "# Example usage\n",
    "for batch_idx, (depth_batch, target_batch) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx+1} - Depth batch shape: {depth_batch.shape}, Target batch shape: {target_batch.shape}\")\n",
    "    # Depth batch shape: [batch_size, 1, 256, 256]\n",
    "    # Target batch shape: [batch_size, 3] (assuming 'can_pos_quat' has 3 values to predict)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DepthImageRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DepthImageRegressor, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Max Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 512)  # Adjust based on the output size after conv layers\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 3)  # Output layer for 3 real numbers\n",
    "        \n",
    "        # Activation and normalization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU activation and MaxPooling\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.pool(self.relu(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the tensor\n",
    "        x = x.reshape(-1, 256 * 16 * 16)  # Use reshape instead of view\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer (no activation, because we are predicting real numbers)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Output constrained to -1 to 1\n",
    "        return torch.tanh(x)\n",
    "\n",
    "# Hyperparameters\n",
    "#batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = DepthImageRegressor()\n",
    "criterion = nn.MSELoss()#torch.nn.L1Loss() #nn.MSELoss()  # Mean squared error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Example of training loop\n",
    "def train_model(train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, targets in train_loader:\n",
    "            # Move data to appropriate device\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6105804574787617\n",
      "Epoch [2/5], Loss: 0.591122844427824\n",
      "Epoch [3/5], Loss: 0.5854073817133904\n",
      "Epoch [4/5], Loss: 0.5836430505812168\n",
      "Epoch [5/5], Loss: 0.5839631549715996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "train_model(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /research/rs4tmr/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "from my_utils import get_current_time\n",
    "import time\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), f'checkpoints/model_2k_5_norm.pth')\n",
    "print(\"Model saved to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147521/1544542406.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'checkpoints/model_2k_5_norm.pth'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load model\n",
    "model = DepthImageRegressor()\n",
    "model.load_state_dict(torch.load(f'checkpoints/model_2k_5_norm.pth'))\n",
    "model.eval()\n",
    "print(\"Model loaded from model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### controller_config: OSC_POSITION ###\n",
      "#### J PickPlace ####\n",
      "fix_object:False\n",
      "start with grasp lock: True\n",
      "control_freq: 20\n",
      "ignore_done: False\n",
      "Predicted target: [[ 0.09540146 -0.25017262  0.86      ]]\n",
      "GT target: [ 0.00716934 -0.33348435  0.86      ]\n",
      "Predicted target: [[ 0.09540146 -0.25017262  0.86      ]]\n",
      "GT target: [ 0.16245435 -0.25579994  0.86      ]\n",
      "Predicted target: [[ 0.09540146 -0.25017262  0.86      ]]\n",
      "GT target: [ 0.17173935 -0.12628785  0.86      ]\n",
      "Predicted target: [[ 0.09540146 -0.25017262  0.86      ]]\n",
      "GT target: [ 0.21680607 -0.35904104  0.86      ]\n",
      "Predicted target: [[ 0.09540146 -0.25017261  0.86      ]]\n",
      "GT target: [ 0.08706148 -0.21058     0.86      ]\n",
      "Predicted target: [[ 0.09540146 -0.25017262  0.86      ]]\n",
      "GT target: [ 0.21606738 -0.11524788  0.86      ]\n",
      "Predicted target: [[ 0.09540146 -0.25017262  0.86      ]]\n",
      "GT target: [ 0.09135239 -0.27883115  0.86      ]\n",
      "Predicted target: [[ 0.09540146 -0.25017261  0.86      ]]\n",
      "GT target: [ 0.19592014 -0.31104351  0.86      ]\n",
      "Predicted target: [[ 0.09540146 -0.25017262  0.86      ]]\n",
      "GT target: [ 0.12804443 -0.13954942  0.86      ]\n",
      "Predicted target: [[ 0.09540145 -0.25017262  0.86      ]]\n",
      "GT target: [ 0.03849025 -0.41487562  0.86      ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"show_videos\" style=\"border-spacing:0px;\"><tr><td style=\"padding:1px;\"><video controls width=\"256\" height=\"256\" style=\"object-fit:cover;\" loop autoplay muted>\n",
       "      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"/>\n",
       "      This browser does not support the video tag.\n",
       "      </video></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "# 필요한 클래스 및 함수 임포트\n",
    "from cleanrl.cleanrl.ppo_continuous_action import Agent, Args, ppo_make_env\n",
    "import cv2\n",
    "from robosuite.utils.camera_utils import CameraMover\n",
    "\n",
    "visualize = True\n",
    "frames = []\n",
    "\n",
    "# Argument 설정\n",
    "task_id = 'pickplace'\n",
    "seed = 0\n",
    "gamma = 0.99\n",
    "num_episodes = 1\n",
    "render_camera = ['birdview']#,'agentview'] #('frontview', 'birdview', 'agentview', 'robot0_robotview', 'robot0_eye_in_hand')\n",
    "camera_names = render_camera\n",
    "\n",
    "\n",
    "# 환경 생성\n",
    "env = gym.vector.SyncVectorEnv(\n",
    "    [ppo_make_env(\n",
    "        task_id=task_id, \n",
    "        reward_shaping=True,\n",
    "        idx=0, \n",
    "        control_mode=\"OSC_POSITION\",\n",
    "        capture_video=False, \n",
    "        run_name=\"eval\", \n",
    "        gamma= gamma, \n",
    "        active_rewards=\"r\",\n",
    "        active_image=True, \n",
    "        fix_object=False,\n",
    "        wandb_enabled=False,\n",
    "        verbose=False,\n",
    "        control_freq=20,\n",
    "        render_camera=render_camera,\n",
    "        camera_names=camera_names,\n",
    "\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def colorize_depth(frame):\n",
    "    # Assuming the depth image is in float32 and contains values representing distances.\n",
    "    # Normalize the depth image to 0-255 for visualization\n",
    "    min_depth = np.min(frame)\n",
    "    max_depth = np.max(frame)\n",
    "    normalized_depth = 255 * (frame - min_depth) / (max_depth - min_depth)\n",
    "    normalized_depth = normalized_depth.astype(np.uint8)\n",
    "\n",
    "    # Apply a colormap for better visualization (COLORMAP_JET is commonly used)\n",
    "    colorized_depth = cv2.applyColorMap(normalized_depth, cv2.COLORMAP_JET)\n",
    "    return colorized_depth\n",
    "\n",
    "\n",
    "# 디바이스 설정 (cuda가 가능하면 cuda 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    print(\"Using CUDA\")\n",
    "else :\n",
    "    assert device == torch.device(\"cpu\")\n",
    "\n",
    "# 평가 수행\n",
    "total_rewards = []\n",
    "viewer_image_key = 'birdview'+'_depth'\n",
    "\n",
    "# generate samples\n",
    "num_samples = 10\n",
    "\n",
    "model.eval()\n",
    "for i in range(num_samples):\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    image_frame = env.envs[0].image_states[viewer_image_key]\n",
    "    if not viewer_image_key.endswith('depth'):\n",
    "        image_frame = np.array(image_frame[::-1, :, :], dtype=np.uint8)  # numpy 배열로 변환\n",
    "    else:\n",
    "        image_frame = np.array(image_frame[::-1, :, :], dtype=np.float32)\n",
    "\n",
    "    model_input = torch.tensor(image_frame, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "    \n",
    "    predicted_target = model(model_input).cpu().detach().numpy()\n",
    "    predicted_target = (predicted_target * std) + mean\n",
    "\n",
    "    print(f\"Predicted target: {predicted_target}\")\n",
    "    print(f\"GT target: {env.envs[0].sim.data.get_body_xpos('Can_main')}\")\n",
    "\n",
    "    image_frame = colorize_depth(image_frame)\n",
    "    \n",
    "    can_pos = env.envs[0].sim.data.get_body_xpos('Can_main')  # Assuming the object is called 'Can'\n",
    "    can_quat = env.envs[0].sim.data.get_body_xquat('Can_main')\n",
    "    pos_text = f\"Pos: {can_pos}\"\n",
    "    quat_text = f\"Quat: {can_quat}\"\n",
    "    cv2.putText(image_frame, pos_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.2, (255, 255, 255), 1)\n",
    "    cv2.putText(image_frame, quat_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.2, (255, 255, 255), 1)\n",
    "   \n",
    "    \n",
    "    frames.append(image_frame)\n",
    "\n",
    "media.show_video(frames, fps=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs4tmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
